
    
    
    
    
    [{"author":"Monte Carlo","categories":null,"contents":"This is a brief introduction and implementation of GANs through one of its variant form, that is DCGANs\r.\nWhat is DCGANs? GANs is basically a framework where we simultaneously train two models include a generator (use to simulate the data distribution) and a discriminator (use to distinguish a sample is from the data distribution or the generator). This can be consider as a minimax game, where we train D (discriminator) to maximize the probability of assigning the correct label to both training examples and samples from G (generator) and we simultaneously train G to minimize (z is a random noise): $$ min_G, max_D V(D, G) = E_{x \\sim pdata(x)}[log D(x)] + E_{z \\sim p_z(z)}[log (1 - D(G(z)))] $$","date":"2023-10-18T00:00:00Z","permalink":"https://monte-cario.github.io/blog/gans2/","tags":null,"title":"GANs: Generative Adversarial Networks"},{"author":"Monte Carlo","categories":null,"contents":"Estimators and Bias Point Estimation Point estimation is the attempt to provide the single “best” prediction of some quantity of interest. In general, the quantity of interest can be a single parameter or a vector of parameters in some parametric model, it can also be a whole function.\nWe take the frequentist perspective on statistics. That is, we assume that the true parameter value \\(\\theta\\) is fixed but unknown, while the point estimate \\(\\hat \\theta\\) is a function of the data.","date":"2023-08-18T00:00:00Z","permalink":"https://monte-cario.github.io/blog/jandh/","tags":["ML Background"],"title":"Machine Learning Basics"},{"author":"Monte Carlo","categories":null,"contents":"Information theory Intuition The basic intuition behind information theory is that learning that an unlikely event has occurred is more informative than learning that a likely event has occurred. A message saying “the sun rose this morning” is so uninformative as to be unnecessary to send, but a message saying “there was a solar eclipse this morning” is very informative.\nIndependent events should have additive information. For example, the information gained from the event that a tossed coin has come up as heads twice is double than the event a tossed coin has come up as heads once.","date":"2023-08-18T00:00:00Z","permalink":"https://monte-cario.github.io/blog/pait/","tags":["Probability background"],"title":"Probability and Information Theory"},{"author":"Monte Carlo","categories":null,"contents":"The Moore-Penrose Pseudoinverse Definition As we learned in linear algebra, we can not inverse a matrix if it is not square. For example, if we want to solve the equation:\n$$ \\boldsymbol{Ax=y} $$\nIf the matrix \\(\\boldsymbol{A}\\) is invertible then the solution is \\(\\boldsymbol{x = A^{-1}y}\\). What if \\(\\boldsymbol{A}\\) is not invertible, we can define the pseudoinverse of A as:\n$$ \\boldsymbol{A^+ = \\lim_{\\delta\\to 0}(A^{\\top}A + \\delta^2I)^{-1}A^{\\top}} $$\nwhere \\(\\boldsymbol{\\delta}\\) is added to prevent \\(\\boldsymbol{A^{\\top}A}\\) reach zero.","date":"2023-08-17T00:00:00Z","permalink":"https://monte-cario.github.io/blog/la_nc/","tags":["Linear Algebra"],"title":"Linear Algebra and Numerical Computation"},{"author":null,"categories":null,"contents":"Deep Learning Date: October 17, 2023 Description: Foundation of DL Status: Not started Tag: Learning\nLinear Algebra The Moore-Penrose Pseudoinverse As we learned in linear algebra, we can not inverse a matrix if it is not square. For example, if we want to solve the equation:\n$$ \\boldsymbol{Ax=y} $$\nIf the matrix $\\boldsymbol{A}$ is invertible then the solution is $\\boldsymbol{x = A^{-1}y}$. What if $\\boldsymbol{A}$ is not invertible, we can define the pseudoinverse of A as:","date":null,"permalink":"https://monte-cario.github.io/blog/deep-learning-2784a20bb0664cb998863d2c1c703e18/","tags":null,"title":""}]