<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Monte-Carlo</title>
    <link>https://monte-cario.github.io/</link>
    <description>Recent content on Monte-Carlo</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Wed, 18 Oct 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://monte-cario.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>GANs: Generative Adversarial Networks</title>
      <link>https://monte-cario.github.io/blog/gans2/</link>
      <pubDate>Wed, 18 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://monte-cario.github.io/blog/gans2/</guid>
      <description>This is a brief introduction and implementation of GANs through one of its variant form, that is DCGANs.
What is DCGANs? GANs is basically a framework where we simultaneously train two models include a generator (use to simulate the data distribution) and a discriminator (use to distinguish a sample is from the data distribution or the generator). This can be consider as a minimax game, where we train D (discriminator) to maximize the probability of assigning the correct label to both training examples and samples from G (generator) and we simultaneously train G to minimize (z is a random noise): $$ min_G, max_D V(D, G) = E_{x \sim pdata(x)}[log D(x)] + E_{z \sim p_z(z)}[log (1 - D(G(z)))] $$</description>
    </item>
    
    <item>
      <title>Machine Learning Basics</title>
      <link>https://monte-cario.github.io/blog/jandh/</link>
      <pubDate>Fri, 18 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>https://monte-cario.github.io/blog/jandh/</guid>
      <description>Estimators and Bias Point Estimation Point estimation is the attempt to provide the single “best” prediction of some quantity of interest. In general, the quantity of interest can be a single parameter or a vector of parameters in some parametric model, it can also be a whole function.
We take the frequentist perspective on statistics. That is, we assume that the true parameter value \(\theta\) is fixed but unknown, while the point estimate \(\hat \theta\) is a function of the data.</description>
    </item>
    
    <item>
      <title>Probability and Information Theory</title>
      <link>https://monte-cario.github.io/blog/pait/</link>
      <pubDate>Fri, 18 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>https://monte-cario.github.io/blog/pait/</guid>
      <description>Information theory Intuition The basic intuition behind information theory is that learning that an unlikely event has occurred is more informative than learning that a likely event has occurred. A message saying “the sun rose this morning” is so uninformative as to be unnecessary to send, but a message saying “there was a solar eclipse this morning” is very informative.
Independent events should have additive information. For example, the information gained from the event that a tossed coin has come up as heads twice is double than the event a tossed coin has come up as heads once.</description>
    </item>
    
    <item>
      <title>Linear Algebra and Numerical Computation</title>
      <link>https://monte-cario.github.io/blog/la_nc/</link>
      <pubDate>Thu, 17 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>https://monte-cario.github.io/blog/la_nc/</guid>
      <description>The Moore-Penrose Pseudoinverse Definition As we learned in linear algebra, we can not inverse a matrix if it is not square. For example, if we want to solve the equation:
$$ \boldsymbol{Ax=y} $$
If the matrix \(\boldsymbol{A}\) is invertible then the solution is \(\boldsymbol{x = A^{-1}y}\). What if \(\boldsymbol{A}\) is not invertible, we can define the pseudoinverse of A as:
$$ \boldsymbol{A^+ = \lim_{\delta\to 0}(A^{\top}A + \delta^2I)^{-1}A^{\top}} $$
where \(\boldsymbol{\delta}\) is added to prevent \(\boldsymbol{A^{\top}A}\) reach zero.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://monte-cario.github.io/blog/deep-learning-2784a20bb0664cb998863d2c1c703e18/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://monte-cario.github.io/blog/deep-learning-2784a20bb0664cb998863d2c1c703e18/</guid>
      <description>Deep Learning Date: October 17, 2023 Description: Foundation of DL Status: Not started Tag: Learning
Linear Algebra The Moore-Penrose Pseudoinverse As we learned in linear algebra, we can not inverse a matrix if it is not square. For example, if we want to solve the equation:
$$ \boldsymbol{Ax=y} $$
If the matrix $\boldsymbol{A}$ is invertible then the solution is $\boldsymbol{x = A^{-1}y}$. What if $\boldsymbol{A}$ is not invertible, we can define the pseudoinverse of A as:</description>
    </item>
    
  </channel>
</rss>
